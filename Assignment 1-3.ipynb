{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce26696",
   "metadata": {},
   "source": [
    "Note: Use this template to develop your project. Do not change the steps. For each step, you may add additional cells if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd1d64",
   "metadata": {},
   "source": [
    "#### Group Information\n",
    "\n",
    "Group No: \n",
    "\n",
    "- Member 1: MOHAMAD NAZMI BIN HASHIM\n",
    "- Member 2: MIOR MUHAMMAD IRFAN BIN MIOR LATFEE\n",
    "- Member 3: MUHAMMAD HAIQAL BIN RAFIQUZZAMAN\n",
    "- Member 4: MUHAMMAD KHAWARIZMI BIN JEFRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239cfe9",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a04735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line serves as a configuration option for Jupyter Notebook's Jedi autocomplete library\n",
    "%config Completer.use_jedi=False\n",
    "\n",
    "# This lines imports NumPy library that is used to support effective numerical operations and arrays in Python\n",
    "import numpy as np \n",
    "\n",
    "#This line imports the Pandas library, which provides data manipulation and analysis tools in Python\n",
    "import pandas as pd\n",
    "\n",
    "# This line imports the pyplot module from the Matplotlib library, that is used create \n",
    "# various types of plots and visualizations.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba455fd5",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf67cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to enable the data can be worked with\n",
    "# read data from a csv file\n",
    "dataset = pd.read_csv('assignment1_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cea945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.764216</td>\n",
       "      <td>-1.016209</td>\n",
       "      <td>0.149410</td>\n",
       "      <td>-0.050119</td>\n",
       "      <td>-0.578127</td>\n",
       "      <td>6.242514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.763880</td>\n",
       "      <td>-1.159509</td>\n",
       "      <td>-0.721492</td>\n",
       "      <td>-0.654067</td>\n",
       "      <td>-0.431670</td>\n",
       "      <td>-8.118241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.519329</td>\n",
       "      <td>-0.664621</td>\n",
       "      <td>-1.694904</td>\n",
       "      <td>1.339779</td>\n",
       "      <td>0.182764</td>\n",
       "      <td>66.722455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.177388</td>\n",
       "      <td>0.515623</td>\n",
       "      <td>0.135144</td>\n",
       "      <td>-0.647634</td>\n",
       "      <td>-0.405631</td>\n",
       "      <td>-27.716793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.104022</td>\n",
       "      <td>0.749665</td>\n",
       "      <td>-0.939338</td>\n",
       "      <td>-0.090725</td>\n",
       "      <td>-0.639963</td>\n",
       "      <td>8.192075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4        f5   response\n",
       "0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n",
       "1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n",
       "2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n",
       "3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n",
       "4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To retrieve the data in the first five rows\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882a6662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.310133</td>\n",
       "      <td>0.529274</td>\n",
       "      <td>-1.439255</td>\n",
       "      <td>0.724974</td>\n",
       "      <td>0.430063</td>\n",
       "      <td>35.181828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.731895</td>\n",
       "      <td>-0.223302</td>\n",
       "      <td>-1.228191</td>\n",
       "      <td>-2.034934</td>\n",
       "      <td>0.509077</td>\n",
       "      <td>-70.134876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.343181</td>\n",
       "      <td>0.431241</td>\n",
       "      <td>-0.054715</td>\n",
       "      <td>0.945423</td>\n",
       "      <td>-2.474684</td>\n",
       "      <td>42.925478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.391021</td>\n",
       "      <td>0.494147</td>\n",
       "      <td>0.106403</td>\n",
       "      <td>-0.652278</td>\n",
       "      <td>-0.200139</td>\n",
       "      <td>-13.287862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.376168</td>\n",
       "      <td>-0.054266</td>\n",
       "      <td>-0.880176</td>\n",
       "      <td>-0.334246</td>\n",
       "      <td>-0.043447</td>\n",
       "      <td>-6.829767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f3        f4        f5   response\n",
       "995 -0.310133  0.529274 -1.439255  0.724974  0.430063  35.181828\n",
       "996 -0.731895 -0.223302 -1.228191 -2.034934  0.509077 -70.134876\n",
       "997  0.343181  0.431241 -0.054715  0.945423 -2.474684  42.925478\n",
       "998  0.391021  0.494147  0.106403 -0.652278 -0.200139 -13.287862\n",
       "999 -0.376168 -0.054266 -0.880176 -0.334246 -0.043447  -6.829767"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To retrieve the data in the last five rows\n",
    "dataset.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d78175a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.012255</td>\n",
       "      <td>-0.043030</td>\n",
       "      <td>-0.065785</td>\n",
       "      <td>0.039616</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>11.229435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.998816</td>\n",
       "      <td>1.042413</td>\n",
       "      <td>0.982640</td>\n",
       "      <td>1.023960</td>\n",
       "      <td>1.006679</td>\n",
       "      <td>40.028188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.174809</td>\n",
       "      <td>-3.381691</td>\n",
       "      <td>-3.158010</td>\n",
       "      <td>-2.764936</td>\n",
       "      <td>-2.946633</td>\n",
       "      <td>-103.044475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.655282</td>\n",
       "      <td>-0.759477</td>\n",
       "      <td>-0.734505</td>\n",
       "      <td>-0.660802</td>\n",
       "      <td>-0.685371</td>\n",
       "      <td>-16.580272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.001177</td>\n",
       "      <td>-0.038444</td>\n",
       "      <td>-0.049838</td>\n",
       "      <td>-0.006831</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>10.554227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.697331</td>\n",
       "      <td>0.696343</td>\n",
       "      <td>0.591642</td>\n",
       "      <td>0.737806</td>\n",
       "      <td>0.710398</td>\n",
       "      <td>38.485118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.092866</td>\n",
       "      <td>3.534175</td>\n",
       "      <td>3.406115</td>\n",
       "      <td>3.145835</td>\n",
       "      <td>3.007734</td>\n",
       "      <td>157.890314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f1           f2           f3           f4           f5  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.012255    -0.043030    -0.065785     0.039616     0.008074   \n",
       "std       0.998816     1.042413     0.982640     1.023960     1.006679   \n",
       "min      -3.174809    -3.381691    -3.158010    -2.764936    -2.946633   \n",
       "25%      -0.655282    -0.759477    -0.734505    -0.660802    -0.685371   \n",
       "50%      -0.001177    -0.038444    -0.049838    -0.006831    -0.000368   \n",
       "75%       0.697331     0.696343     0.591642     0.737806     0.710398   \n",
       "max       3.092866     3.534175     3.406115     3.145835     3.007734   \n",
       "\n",
       "          response  \n",
       "count  1000.000000  \n",
       "mean     11.229435  \n",
       "std      40.028188  \n",
       "min    -103.044475  \n",
       "25%     -16.580272  \n",
       "50%      10.554227  \n",
       "75%      38.485118  \n",
       "max     157.890314  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the quick overview of the data distribution and the dain insights about dataset\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25576a90",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac2d37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    \"\"\" Pass two arguments\n",
    "    Arguments:\n",
    "        y: responses\n",
    "        yhat: predicted value\n",
    "    Returns:\n",
    "        loss: loss value\n",
    "    \"\"\"\n",
    "    #calculate the mean squared error for the loss value\n",
    "    #loss = ((y - yhat)**2).mean()\n",
    "    \n",
    "    #calculate the mean absolute error for the loss value\n",
    "    error = yhat - y\n",
    "    absolute_error = np.absolute(error)\n",
    "    total_absolute_error = np.sum(absolute_error)\n",
    "    loss = total_absolute_error/y.size\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7a8ef",
   "metadata": {},
   "source": [
    "#### Define function to perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4d36037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    \"\"\" Pass two arguments\n",
    "    Arguments:\n",
    "        w: weights\n",
    "        X: input features\n",
    "    Returns:\n",
    "        yhat: predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    yhat = x.dot(w)\n",
    "   \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb47e6",
   "metadata": {},
   "source": [
    "#### Define function for model training\n",
    "Display the training loss value for each epoch of the training loop. The displayed value must be in 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64780369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    " \n",
    "    \n",
    "    # Multivariate Linear Regression using Gradient Descent\n",
    "    n = X.shape[1] # get number of columns for X\n",
    "    w = np.ones(n) # set the number of number of weights from n\n",
    "    yhat = predict(w,X) # Calculate hypothesis using formula\n",
    "    hist_loss = []\n",
    "\n",
    "    # Gradient descent algorithm\n",
    "    hist_loss = np.ones(max_epoch)\n",
    "    for i in range (0, max_epoch):\n",
    "        w[0] = w[0] - (alpha / X.shape[0]) * sum(yhat-y)\n",
    "        for j in range(1, n) :\n",
    "            w[j] = w[j] - (alpha/ X.shape[0]) * sum((yhat-y) * X[:, j])\n",
    "        yhat = predict(w,X)\n",
    "        hist_loss[i] = loss_fn(y, yhat) #call the loss_fn function\n",
    "        print(f\"Epoch {i+1}: Loss = {hist_loss[i]:.3f}\") #display loss value for each epoch\n",
    "\n",
    "    \n",
    "\n",
    "    return w, hist_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ac3fc",
   "metadata": {},
   "source": [
    "#### Split the dataset\n",
    "The ratio of training and test is 8:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d150f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn provides a tool called train_test_split that assists in dividing \n",
    "# a dataset into training and testing subsets. \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6ab9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line selects all the rows of the dataset and all columns except the last column\n",
    "X = dataset.iloc[:, :-1] #assigning features to a variable\n",
    "\n",
    "# This line selects all the rows of the dataset and only the last column\n",
    "y = dataset.iloc[:, -1]  #assigning target to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a623a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0] # number of samples =1000 rows\n",
    "ones =np.ones((m,1))\n",
    "X = np. concatenate((ones, X), axis=1) # Nuw x with X0's =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be939cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "# Note: Random state is used to split the same row constantly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b50633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab48de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train set: \n",
      "X_train set:  (800, 6) \n",
      "y_train set:  (800,)\n",
      "\n",
      "The shape of test set: \n",
      "X_test set:  (200, 6) \n",
      "y_test set:  (200,)\n"
     ]
    }
   ],
   "source": [
    "#Display number of rows and columns for the datasets\n",
    "print('The shape of train set: ')\n",
    "print('X_train set: ', X_train.shape, '\\ny_train set: ', y_train.shape)\n",
    "print('')\n",
    "print('The shape of test set: ')\n",
    "print('X_test set: ', X_test.shape, '\\ny_test set: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d5638",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a3e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 28.665\n",
      "Epoch 2: Loss = 25.681\n",
      "Epoch 3: Loss = 23.018\n",
      "Epoch 4: Loss = 20.645\n",
      "Epoch 5: Loss = 18.538\n",
      "Epoch 6: Loss = 16.664\n",
      "Epoch 7: Loss = 14.999\n",
      "Epoch 8: Loss = 13.522\n",
      "Epoch 9: Loss = 12.221\n",
      "Epoch 10: Loss = 11.071\n",
      "Epoch 11: Loss = 10.059\n",
      "Epoch 12: Loss = 9.164\n",
      "Epoch 13: Loss = 8.385\n",
      "Epoch 14: Loss = 7.708\n",
      "Epoch 15: Loss = 7.117\n",
      "Epoch 16: Loss = 6.605\n",
      "Epoch 17: Loss = 6.164\n",
      "Epoch 18: Loss = 5.785\n",
      "Epoch 19: Loss = 5.460\n",
      "Epoch 20: Loss = 5.189\n",
      "Epoch 21: Loss = 4.969\n",
      "Epoch 22: Loss = 4.785\n",
      "Epoch 23: Loss = 4.627\n",
      "Epoch 24: Loss = 4.494\n",
      "Epoch 25: Loss = 4.391\n",
      "Epoch 26: Loss = 4.307\n",
      "Epoch 27: Loss = 4.242\n",
      "Epoch 28: Loss = 4.187\n",
      "Epoch 29: Loss = 4.144\n",
      "Epoch 30: Loss = 4.110\n",
      "Epoch 31: Loss = 4.084\n",
      "Epoch 32: Loss = 4.064\n",
      "Epoch 33: Loss = 4.048\n",
      "Epoch 34: Loss = 4.037\n",
      "Epoch 35: Loss = 4.029\n",
      "Epoch 36: Loss = 4.024\n",
      "Epoch 37: Loss = 4.020\n",
      "Epoch 38: Loss = 4.017\n",
      "Epoch 39: Loss = 4.015\n",
      "Epoch 40: Loss = 4.014\n",
      "Epoch 41: Loss = 4.013\n",
      "Epoch 42: Loss = 4.012\n",
      "Epoch 43: Loss = 4.012\n",
      "Epoch 44: Loss = 4.012\n",
      "Epoch 45: Loss = 4.012\n",
      "Epoch 46: Loss = 4.012\n",
      "Epoch 47: Loss = 4.012\n",
      "Epoch 48: Loss = 4.012\n",
      "Epoch 49: Loss = 4.013\n",
      "Epoch 50: Loss = 4.013\n",
      "Epoch 51: Loss = 4.013\n",
      "Epoch 52: Loss = 4.014\n",
      "Epoch 53: Loss = 4.014\n",
      "Epoch 54: Loss = 4.014\n",
      "Epoch 55: Loss = 4.015\n",
      "Epoch 56: Loss = 4.015\n",
      "Epoch 57: Loss = 4.015\n",
      "Epoch 58: Loss = 4.015\n",
      "Epoch 59: Loss = 4.015\n",
      "Epoch 60: Loss = 4.016\n",
      "Epoch 61: Loss = 4.016\n",
      "Epoch 62: Loss = 4.016\n",
      "Epoch 63: Loss = 4.016\n",
      "Epoch 64: Loss = 4.016\n",
      "Epoch 65: Loss = 4.016\n",
      "Epoch 66: Loss = 4.016\n",
      "Epoch 67: Loss = 4.016\n",
      "Epoch 68: Loss = 4.016\n",
      "Epoch 69: Loss = 4.017\n",
      "Epoch 70: Loss = 4.017\n",
      "Epoch 71: Loss = 4.017\n",
      "Epoch 72: Loss = 4.017\n",
      "Epoch 73: Loss = 4.017\n",
      "Epoch 74: Loss = 4.017\n",
      "Epoch 75: Loss = 4.017\n",
      "Epoch 76: Loss = 4.017\n",
      "Epoch 77: Loss = 4.017\n",
      "Epoch 78: Loss = 4.017\n",
      "Epoch 79: Loss = 4.017\n",
      "Epoch 80: Loss = 4.017\n",
      "Epoch 81: Loss = 4.017\n",
      "Epoch 82: Loss = 4.017\n",
      "Epoch 83: Loss = 4.017\n",
      "Epoch 84: Loss = 4.017\n",
      "Epoch 85: Loss = 4.017\n",
      "Epoch 86: Loss = 4.017\n",
      "Epoch 87: Loss = 4.017\n",
      "Epoch 88: Loss = 4.017\n",
      "Epoch 89: Loss = 4.017\n",
      "Epoch 90: Loss = 4.017\n",
      "Epoch 91: Loss = 4.017\n",
      "Epoch 92: Loss = 4.017\n",
      "Epoch 93: Loss = 4.017\n",
      "Epoch 94: Loss = 4.017\n",
      "Epoch 95: Loss = 4.017\n",
      "Epoch 96: Loss = 4.017\n",
      "Epoch 97: Loss = 4.017\n",
      "Epoch 98: Loss = 4.017\n",
      "Epoch 99: Loss = 4.017\n",
      "Epoch 100: Loss = 4.017\n",
      "Epoch 101: Loss = 4.017\n",
      "Epoch 102: Loss = 4.017\n",
      "Epoch 103: Loss = 4.017\n",
      "Epoch 104: Loss = 4.017\n",
      "Epoch 105: Loss = 4.017\n",
      "Epoch 106: Loss = 4.017\n",
      "Epoch 107: Loss = 4.017\n",
      "Epoch 108: Loss = 4.017\n",
      "Epoch 109: Loss = 4.017\n",
      "Epoch 110: Loss = 4.017\n",
      "Epoch 111: Loss = 4.017\n",
      "Epoch 112: Loss = 4.017\n",
      "Epoch 113: Loss = 4.017\n",
      "Epoch 114: Loss = 4.017\n",
      "Epoch 115: Loss = 4.017\n",
      "Epoch 116: Loss = 4.017\n",
      "Epoch 117: Loss = 4.017\n",
      "Epoch 118: Loss = 4.017\n",
      "Epoch 119: Loss = 4.017\n",
      "Epoch 120: Loss = 4.017\n",
      "Epoch 121: Loss = 4.017\n",
      "Epoch 122: Loss = 4.017\n",
      "Epoch 123: Loss = 4.017\n",
      "Epoch 124: Loss = 4.017\n",
      "Epoch 125: Loss = 4.017\n",
      "Epoch 126: Loss = 4.017\n",
      "Epoch 127: Loss = 4.017\n",
      "Epoch 128: Loss = 4.017\n",
      "Epoch 129: Loss = 4.017\n",
      "Epoch 130: Loss = 4.017\n",
      "Epoch 131: Loss = 4.017\n",
      "Epoch 132: Loss = 4.017\n",
      "Epoch 133: Loss = 4.017\n",
      "Epoch 134: Loss = 4.017\n",
      "Epoch 135: Loss = 4.017\n",
      "Epoch 136: Loss = 4.017\n",
      "Epoch 137: Loss = 4.017\n",
      "Epoch 138: Loss = 4.017\n",
      "Epoch 139: Loss = 4.017\n",
      "Epoch 140: Loss = 4.017\n",
      "Epoch 141: Loss = 4.017\n",
      "Epoch 142: Loss = 4.017\n",
      "Epoch 143: Loss = 4.017\n",
      "Epoch 144: Loss = 4.017\n",
      "Epoch 145: Loss = 4.017\n",
      "Epoch 146: Loss = 4.017\n",
      "Epoch 147: Loss = 4.017\n",
      "Epoch 148: Loss = 4.017\n",
      "Epoch 149: Loss = 4.017\n",
      "Epoch 150: Loss = 4.017\n",
      "Epoch 151: Loss = 4.017\n",
      "Epoch 152: Loss = 4.017\n",
      "Epoch 153: Loss = 4.017\n",
      "Epoch 154: Loss = 4.017\n",
      "Epoch 155: Loss = 4.017\n",
      "Epoch 156: Loss = 4.017\n",
      "Epoch 157: Loss = 4.017\n",
      "Epoch 158: Loss = 4.017\n",
      "Epoch 159: Loss = 4.017\n",
      "Epoch 160: Loss = 4.017\n",
      "Epoch 161: Loss = 4.017\n",
      "Epoch 162: Loss = 4.017\n",
      "Epoch 163: Loss = 4.017\n",
      "Epoch 164: Loss = 4.017\n",
      "Epoch 165: Loss = 4.017\n",
      "Epoch 166: Loss = 4.017\n",
      "Epoch 167: Loss = 4.017\n",
      "Epoch 168: Loss = 4.017\n",
      "Epoch 169: Loss = 4.017\n",
      "Epoch 170: Loss = 4.017\n",
      "Epoch 171: Loss = 4.017\n",
      "Epoch 172: Loss = 4.017\n",
      "Epoch 173: Loss = 4.017\n",
      "Epoch 174: Loss = 4.017\n",
      "Epoch 175: Loss = 4.017\n",
      "Epoch 176: Loss = 4.017\n",
      "Epoch 177: Loss = 4.017\n",
      "Epoch 178: Loss = 4.017\n",
      "Epoch 179: Loss = 4.017\n",
      "Epoch 180: Loss = 4.017\n",
      "Epoch 181: Loss = 4.017\n",
      "Epoch 182: Loss = 4.017\n",
      "Epoch 183: Loss = 4.017\n",
      "Epoch 184: Loss = 4.017\n",
      "Epoch 185: Loss = 4.017\n",
      "Epoch 186: Loss = 4.017\n",
      "Epoch 187: Loss = 4.017\n",
      "Epoch 188: Loss = 4.017\n",
      "Epoch 189: Loss = 4.017\n",
      "Epoch 190: Loss = 4.017\n",
      "Epoch 191: Loss = 4.017\n",
      "Epoch 192: Loss = 4.017\n",
      "Epoch 193: Loss = 4.017\n",
      "Epoch 194: Loss = 4.017\n",
      "Epoch 195: Loss = 4.017\n",
      "Epoch 196: Loss = 4.017\n",
      "Epoch 197: Loss = 4.017\n",
      "Epoch 198: Loss = 4.017\n",
      "Epoch 199: Loss = 4.017\n",
      "Epoch 200: Loss = 4.017\n",
      "Epoch 201: Loss = 4.017\n",
      "Epoch 202: Loss = 4.017\n",
      "Epoch 203: Loss = 4.017\n",
      "Epoch 204: Loss = 4.017\n",
      "Epoch 205: Loss = 4.017\n",
      "Epoch 206: Loss = 4.017\n",
      "Epoch 207: Loss = 4.017\n",
      "Epoch 208: Loss = 4.017\n",
      "Epoch 209: Loss = 4.017\n",
      "Epoch 210: Loss = 4.017\n",
      "Epoch 211: Loss = 4.017\n",
      "Epoch 212: Loss = 4.017\n",
      "Epoch 213: Loss = 4.017\n",
      "Epoch 214: Loss = 4.017\n",
      "Epoch 215: Loss = 4.017\n",
      "Epoch 216: Loss = 4.017\n",
      "Epoch 217: Loss = 4.017\n",
      "Epoch 218: Loss = 4.017\n",
      "Epoch 219: Loss = 4.017\n",
      "Epoch 220: Loss = 4.017\n",
      "Epoch 221: Loss = 4.017\n",
      "Epoch 222: Loss = 4.017\n",
      "Epoch 223: Loss = 4.017\n",
      "Epoch 224: Loss = 4.017\n",
      "Epoch 225: Loss = 4.017\n",
      "Epoch 226: Loss = 4.017\n",
      "Epoch 227: Loss = 4.017\n",
      "Epoch 228: Loss = 4.017\n",
      "Epoch 229: Loss = 4.017\n",
      "Epoch 230: Loss = 4.017\n",
      "Epoch 231: Loss = 4.017\n",
      "Epoch 232: Loss = 4.017\n",
      "Epoch 233: Loss = 4.017\n",
      "Epoch 234: Loss = 4.017\n",
      "Epoch 235: Loss = 4.017\n",
      "Epoch 236: Loss = 4.017\n",
      "Epoch 237: Loss = 4.017\n",
      "Epoch 238: Loss = 4.017\n",
      "Epoch 239: Loss = 4.017\n",
      "Epoch 240: Loss = 4.017\n",
      "Epoch 241: Loss = 4.017\n",
      "Epoch 242: Loss = 4.017\n",
      "Epoch 243: Loss = 4.017\n",
      "Epoch 244: Loss = 4.017\n",
      "Epoch 245: Loss = 4.017\n",
      "Epoch 246: Loss = 4.017\n",
      "Epoch 247: Loss = 4.017\n",
      "Epoch 248: Loss = 4.017\n",
      "Epoch 249: Loss = 4.017\n",
      "Epoch 250: Loss = 4.017\n",
      "Epoch 251: Loss = 4.017\n",
      "Epoch 252: Loss = 4.017\n",
      "Epoch 253: Loss = 4.017\n",
      "Epoch 254: Loss = 4.017\n",
      "Epoch 255: Loss = 4.017\n",
      "Epoch 256: Loss = 4.017\n",
      "Epoch 257: Loss = 4.017\n",
      "Epoch 258: Loss = 4.017\n",
      "Epoch 259: Loss = 4.017\n",
      "Epoch 260: Loss = 4.017\n",
      "Epoch 261: Loss = 4.017\n",
      "Epoch 262: Loss = 4.017\n",
      "Epoch 263: Loss = 4.017\n",
      "Epoch 264: Loss = 4.017\n",
      "Epoch 265: Loss = 4.017\n",
      "Epoch 266: Loss = 4.017\n",
      "Epoch 267: Loss = 4.017\n",
      "Epoch 268: Loss = 4.017\n",
      "Epoch 269: Loss = 4.017\n",
      "Epoch 270: Loss = 4.017\n",
      "Epoch 271: Loss = 4.017\n",
      "Epoch 272: Loss = 4.017\n",
      "Epoch 273: Loss = 4.017\n",
      "Epoch 274: Loss = 4.017\n",
      "Epoch 275: Loss = 4.017\n",
      "Epoch 276: Loss = 4.017\n",
      "Epoch 277: Loss = 4.017\n",
      "Epoch 278: Loss = 4.017\n",
      "Epoch 279: Loss = 4.017\n",
      "Epoch 280: Loss = 4.017\n",
      "Epoch 281: Loss = 4.017\n",
      "Epoch 282: Loss = 4.017\n",
      "Epoch 283: Loss = 4.017\n",
      "Epoch 284: Loss = 4.017\n",
      "Epoch 285: Loss = 4.017\n",
      "Epoch 286: Loss = 4.017\n",
      "Epoch 287: Loss = 4.017\n",
      "Epoch 288: Loss = 4.017\n",
      "Epoch 289: Loss = 4.017\n",
      "Epoch 290: Loss = 4.017\n",
      "Epoch 291: Loss = 4.017\n",
      "Epoch 292: Loss = 4.017\n",
      "Epoch 293: Loss = 4.017\n",
      "Epoch 294: Loss = 4.017\n",
      "Epoch 295: Loss = 4.017\n",
      "Epoch 296: Loss = 4.017\n",
      "Epoch 297: Loss = 4.017\n",
      "Epoch 298: Loss = 4.017\n",
      "Epoch 299: Loss = 4.017\n",
      "Epoch 300: Loss = 4.017\n",
      "Epoch 301: Loss = 4.017\n",
      "Epoch 302: Loss = 4.017\n",
      "Epoch 303: Loss = 4.017\n",
      "Epoch 304: Loss = 4.017\n",
      "Epoch 305: Loss = 4.017\n",
      "Epoch 306: Loss = 4.017\n",
      "Epoch 307: Loss = 4.017\n",
      "Epoch 308: Loss = 4.017\n",
      "Epoch 309: Loss = 4.017\n",
      "Epoch 310: Loss = 4.017\n",
      "Epoch 311: Loss = 4.017\n",
      "Epoch 312: Loss = 4.017\n",
      "Epoch 313: Loss = 4.017\n",
      "Epoch 314: Loss = 4.017\n",
      "Epoch 315: Loss = 4.017\n",
      "Epoch 316: Loss = 4.017\n",
      "Epoch 317: Loss = 4.017\n",
      "Epoch 318: Loss = 4.017\n",
      "Epoch 319: Loss = 4.017\n",
      "Epoch 320: Loss = 4.017\n",
      "Epoch 321: Loss = 4.017\n",
      "Epoch 322: Loss = 4.017\n",
      "Epoch 323: Loss = 4.017\n",
      "Epoch 324: Loss = 4.017\n",
      "Epoch 325: Loss = 4.017\n",
      "Epoch 326: Loss = 4.017\n",
      "Epoch 327: Loss = 4.017\n",
      "Epoch 328: Loss = 4.017\n",
      "Epoch 329: Loss = 4.017\n",
      "Epoch 330: Loss = 4.017\n",
      "Epoch 331: Loss = 4.017\n",
      "Epoch 332: Loss = 4.017\n",
      "Epoch 333: Loss = 4.017\n",
      "Epoch 334: Loss = 4.017\n",
      "Epoch 335: Loss = 4.017\n",
      "Epoch 336: Loss = 4.017\n",
      "Epoch 337: Loss = 4.017\n",
      "Epoch 338: Loss = 4.017\n",
      "Epoch 339: Loss = 4.017\n",
      "Epoch 340: Loss = 4.017\n",
      "Epoch 341: Loss = 4.017\n",
      "Epoch 342: Loss = 4.017\n",
      "Epoch 343: Loss = 4.017\n",
      "Epoch 344: Loss = 4.017\n",
      "Epoch 345: Loss = 4.017\n",
      "Epoch 346: Loss = 4.017\n",
      "Epoch 347: Loss = 4.017\n",
      "Epoch 348: Loss = 4.017\n",
      "Epoch 349: Loss = 4.017\n",
      "Epoch 350: Loss = 4.017\n",
      "Epoch 351: Loss = 4.017\n",
      "Epoch 352: Loss = 4.017\n",
      "Epoch 353: Loss = 4.017\n",
      "Epoch 354: Loss = 4.017\n",
      "Epoch 355: Loss = 4.017\n",
      "Epoch 356: Loss = 4.017\n",
      "Epoch 357: Loss = 4.017\n",
      "Epoch 358: Loss = 4.017\n",
      "Epoch 359: Loss = 4.017\n",
      "Epoch 360: Loss = 4.017\n",
      "Epoch 361: Loss = 4.017\n",
      "Epoch 362: Loss = 4.017\n",
      "Epoch 363: Loss = 4.017\n",
      "Epoch 364: Loss = 4.017\n",
      "Epoch 365: Loss = 4.017\n",
      "Epoch 366: Loss = 4.017\n",
      "Epoch 367: Loss = 4.017\n",
      "Epoch 368: Loss = 4.017\n",
      "Epoch 369: Loss = 4.017\n",
      "Epoch 370: Loss = 4.017\n",
      "Epoch 371: Loss = 4.017\n",
      "Epoch 372: Loss = 4.017\n",
      "Epoch 373: Loss = 4.017\n",
      "Epoch 374: Loss = 4.017\n",
      "Epoch 375: Loss = 4.017\n",
      "Epoch 376: Loss = 4.017\n",
      "Epoch 377: Loss = 4.017\n",
      "Epoch 378: Loss = 4.017\n",
      "Epoch 379: Loss = 4.017\n",
      "Epoch 380: Loss = 4.017\n",
      "Epoch 381: Loss = 4.017\n",
      "Epoch 382: Loss = 4.017\n",
      "Epoch 383: Loss = 4.017\n",
      "Epoch 384: Loss = 4.017\n",
      "Epoch 385: Loss = 4.017\n",
      "Epoch 386: Loss = 4.017\n",
      "Epoch 387: Loss = 4.017\n",
      "Epoch 388: Loss = 4.017\n",
      "Epoch 389: Loss = 4.017\n",
      "Epoch 390: Loss = 4.017\n",
      "Epoch 391: Loss = 4.017\n",
      "Epoch 392: Loss = 4.017\n",
      "Epoch 393: Loss = 4.017\n",
      "Epoch 394: Loss = 4.017\n",
      "Epoch 395: Loss = 4.017\n",
      "Epoch 396: Loss = 4.017\n",
      "Epoch 397: Loss = 4.017\n",
      "Epoch 398: Loss = 4.017\n",
      "Epoch 399: Loss = 4.017\n",
      "Epoch 400: Loss = 4.017\n",
      "Epoch 401: Loss = 4.017\n",
      "Epoch 402: Loss = 4.017\n",
      "Epoch 403: Loss = 4.017\n",
      "Epoch 404: Loss = 4.017\n",
      "Epoch 405: Loss = 4.017\n",
      "Epoch 406: Loss = 4.017\n",
      "Epoch 407: Loss = 4.017\n",
      "Epoch 408: Loss = 4.017\n",
      "Epoch 409: Loss = 4.017\n",
      "Epoch 410: Loss = 4.017\n",
      "Epoch 411: Loss = 4.017\n",
      "Epoch 412: Loss = 4.017\n",
      "Epoch 413: Loss = 4.017\n",
      "Epoch 414: Loss = 4.017\n",
      "Epoch 415: Loss = 4.017\n",
      "Epoch 416: Loss = 4.017\n",
      "Epoch 417: Loss = 4.017\n",
      "Epoch 418: Loss = 4.017\n",
      "Epoch 419: Loss = 4.017\n",
      "Epoch 420: Loss = 4.017\n",
      "Epoch 421: Loss = 4.017\n",
      "Epoch 422: Loss = 4.017\n",
      "Epoch 423: Loss = 4.017\n",
      "Epoch 424: Loss = 4.017\n",
      "Epoch 425: Loss = 4.017\n",
      "Epoch 426: Loss = 4.017\n",
      "Epoch 427: Loss = 4.017\n",
      "Epoch 428: Loss = 4.017\n",
      "Epoch 429: Loss = 4.017\n",
      "Epoch 430: Loss = 4.017\n",
      "Epoch 431: Loss = 4.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 432: Loss = 4.017\n",
      "Epoch 433: Loss = 4.017\n",
      "Epoch 434: Loss = 4.017\n",
      "Epoch 435: Loss = 4.017\n",
      "Epoch 436: Loss = 4.017\n",
      "Epoch 437: Loss = 4.017\n",
      "Epoch 438: Loss = 4.017\n",
      "Epoch 439: Loss = 4.017\n",
      "Epoch 440: Loss = 4.017\n",
      "Epoch 441: Loss = 4.017\n",
      "Epoch 442: Loss = 4.017\n",
      "Epoch 443: Loss = 4.017\n",
      "Epoch 444: Loss = 4.017\n",
      "Epoch 445: Loss = 4.017\n",
      "Epoch 446: Loss = 4.017\n",
      "Epoch 447: Loss = 4.017\n",
      "Epoch 448: Loss = 4.017\n",
      "Epoch 449: Loss = 4.017\n",
      "Epoch 450: Loss = 4.017\n",
      "Epoch 451: Loss = 4.017\n",
      "Epoch 452: Loss = 4.017\n",
      "Epoch 453: Loss = 4.017\n",
      "Epoch 454: Loss = 4.017\n",
      "Epoch 455: Loss = 4.017\n",
      "Epoch 456: Loss = 4.017\n",
      "Epoch 457: Loss = 4.017\n",
      "Epoch 458: Loss = 4.017\n",
      "Epoch 459: Loss = 4.017\n",
      "Epoch 460: Loss = 4.017\n",
      "Epoch 461: Loss = 4.017\n",
      "Epoch 462: Loss = 4.017\n",
      "Epoch 463: Loss = 4.017\n",
      "Epoch 464: Loss = 4.017\n",
      "Epoch 465: Loss = 4.017\n",
      "Epoch 466: Loss = 4.017\n",
      "Epoch 467: Loss = 4.017\n",
      "Epoch 468: Loss = 4.017\n",
      "Epoch 469: Loss = 4.017\n",
      "Epoch 470: Loss = 4.017\n",
      "Epoch 471: Loss = 4.017\n",
      "Epoch 472: Loss = 4.017\n",
      "Epoch 473: Loss = 4.017\n",
      "Epoch 474: Loss = 4.017\n",
      "Epoch 475: Loss = 4.017\n",
      "Epoch 476: Loss = 4.017\n",
      "Epoch 477: Loss = 4.017\n",
      "Epoch 478: Loss = 4.017\n",
      "Epoch 479: Loss = 4.017\n",
      "Epoch 480: Loss = 4.017\n",
      "Epoch 481: Loss = 4.017\n",
      "Epoch 482: Loss = 4.017\n",
      "Epoch 483: Loss = 4.017\n",
      "Epoch 484: Loss = 4.017\n",
      "Epoch 485: Loss = 4.017\n",
      "Epoch 486: Loss = 4.017\n",
      "Epoch 487: Loss = 4.017\n",
      "Epoch 488: Loss = 4.017\n",
      "Epoch 489: Loss = 4.017\n",
      "Epoch 490: Loss = 4.017\n",
      "Epoch 491: Loss = 4.017\n",
      "Epoch 492: Loss = 4.017\n",
      "Epoch 493: Loss = 4.017\n",
      "Epoch 494: Loss = 4.017\n",
      "Epoch 495: Loss = 4.017\n",
      "Epoch 496: Loss = 4.017\n",
      "Epoch 497: Loss = 4.017\n",
      "Epoch 498: Loss = 4.017\n",
      "Epoch 499: Loss = 4.017\n",
      "Epoch 500: Loss = 4.017\n",
      "Epoch 501: Loss = 4.017\n",
      "Epoch 502: Loss = 4.017\n",
      "Epoch 503: Loss = 4.017\n",
      "Epoch 504: Loss = 4.017\n",
      "Epoch 505: Loss = 4.017\n",
      "Epoch 506: Loss = 4.017\n",
      "Epoch 507: Loss = 4.017\n",
      "Epoch 508: Loss = 4.017\n",
      "Epoch 509: Loss = 4.017\n",
      "Epoch 510: Loss = 4.017\n",
      "Epoch 511: Loss = 4.017\n",
      "Epoch 512: Loss = 4.017\n",
      "Epoch 513: Loss = 4.017\n",
      "Epoch 514: Loss = 4.017\n",
      "Epoch 515: Loss = 4.017\n",
      "Epoch 516: Loss = 4.017\n",
      "Epoch 517: Loss = 4.017\n",
      "Epoch 518: Loss = 4.017\n",
      "Epoch 519: Loss = 4.017\n",
      "Epoch 520: Loss = 4.017\n",
      "Epoch 521: Loss = 4.017\n",
      "Epoch 522: Loss = 4.017\n",
      "Epoch 523: Loss = 4.017\n",
      "Epoch 524: Loss = 4.017\n",
      "Epoch 525: Loss = 4.017\n",
      "Epoch 526: Loss = 4.017\n",
      "Epoch 527: Loss = 4.017\n",
      "Epoch 528: Loss = 4.017\n",
      "Epoch 529: Loss = 4.017\n",
      "Epoch 530: Loss = 4.017\n",
      "Epoch 531: Loss = 4.017\n",
      "Epoch 532: Loss = 4.017\n",
      "Epoch 533: Loss = 4.017\n",
      "Epoch 534: Loss = 4.017\n",
      "Epoch 535: Loss = 4.017\n",
      "Epoch 536: Loss = 4.017\n",
      "Epoch 537: Loss = 4.017\n",
      "Epoch 538: Loss = 4.017\n",
      "Epoch 539: Loss = 4.017\n",
      "Epoch 540: Loss = 4.017\n",
      "Epoch 541: Loss = 4.017\n",
      "Epoch 542: Loss = 4.017\n",
      "Epoch 543: Loss = 4.017\n",
      "Epoch 544: Loss = 4.017\n",
      "Epoch 545: Loss = 4.017\n",
      "Epoch 546: Loss = 4.017\n",
      "Epoch 547: Loss = 4.017\n",
      "Epoch 548: Loss = 4.017\n",
      "Epoch 549: Loss = 4.017\n",
      "Epoch 550: Loss = 4.017\n",
      "Epoch 551: Loss = 4.017\n",
      "Epoch 552: Loss = 4.017\n",
      "Epoch 553: Loss = 4.017\n",
      "Epoch 554: Loss = 4.017\n",
      "Epoch 555: Loss = 4.017\n",
      "Epoch 556: Loss = 4.017\n",
      "Epoch 557: Loss = 4.017\n",
      "Epoch 558: Loss = 4.017\n",
      "Epoch 559: Loss = 4.017\n",
      "Epoch 560: Loss = 4.017\n",
      "Epoch 561: Loss = 4.017\n",
      "Epoch 562: Loss = 4.017\n",
      "Epoch 563: Loss = 4.017\n",
      "Epoch 564: Loss = 4.017\n",
      "Epoch 565: Loss = 4.017\n",
      "Epoch 566: Loss = 4.017\n",
      "Epoch 567: Loss = 4.017\n",
      "Epoch 568: Loss = 4.017\n",
      "Epoch 569: Loss = 4.017\n",
      "Epoch 570: Loss = 4.017\n",
      "Epoch 571: Loss = 4.017\n",
      "Epoch 572: Loss = 4.017\n",
      "Epoch 573: Loss = 4.017\n",
      "Epoch 574: Loss = 4.017\n",
      "Epoch 575: Loss = 4.017\n",
      "Epoch 576: Loss = 4.017\n",
      "Epoch 577: Loss = 4.017\n",
      "Epoch 578: Loss = 4.017\n",
      "Epoch 579: Loss = 4.017\n",
      "Epoch 580: Loss = 4.017\n",
      "Epoch 581: Loss = 4.017\n",
      "Epoch 582: Loss = 4.017\n",
      "Epoch 583: Loss = 4.017\n",
      "Epoch 584: Loss = 4.017\n",
      "Epoch 585: Loss = 4.017\n",
      "Epoch 586: Loss = 4.017\n",
      "Epoch 587: Loss = 4.017\n",
      "Epoch 588: Loss = 4.017\n",
      "Epoch 589: Loss = 4.017\n",
      "Epoch 590: Loss = 4.017\n",
      "Epoch 591: Loss = 4.017\n",
      "Epoch 592: Loss = 4.017\n",
      "Epoch 593: Loss = 4.017\n",
      "Epoch 594: Loss = 4.017\n",
      "Epoch 595: Loss = 4.017\n",
      "Epoch 596: Loss = 4.017\n",
      "Epoch 597: Loss = 4.017\n",
      "Epoch 598: Loss = 4.017\n",
      "Epoch 599: Loss = 4.017\n",
      "Epoch 600: Loss = 4.017\n",
      "Epoch 601: Loss = 4.017\n",
      "Epoch 602: Loss = 4.017\n",
      "Epoch 603: Loss = 4.017\n",
      "Epoch 604: Loss = 4.017\n",
      "Epoch 605: Loss = 4.017\n",
      "Epoch 606: Loss = 4.017\n",
      "Epoch 607: Loss = 4.017\n",
      "Epoch 608: Loss = 4.017\n",
      "Epoch 609: Loss = 4.017\n",
      "Epoch 610: Loss = 4.017\n",
      "Epoch 611: Loss = 4.017\n",
      "Epoch 612: Loss = 4.017\n",
      "Epoch 613: Loss = 4.017\n",
      "Epoch 614: Loss = 4.017\n",
      "Epoch 615: Loss = 4.017\n",
      "Epoch 616: Loss = 4.017\n",
      "Epoch 617: Loss = 4.017\n",
      "Epoch 618: Loss = 4.017\n",
      "Epoch 619: Loss = 4.017\n",
      "Epoch 620: Loss = 4.017\n",
      "Epoch 621: Loss = 4.017\n",
      "Epoch 622: Loss = 4.017\n",
      "Epoch 623: Loss = 4.017\n",
      "Epoch 624: Loss = 4.017\n",
      "Epoch 625: Loss = 4.017\n",
      "Epoch 626: Loss = 4.017\n",
      "Epoch 627: Loss = 4.017\n",
      "Epoch 628: Loss = 4.017\n",
      "Epoch 629: Loss = 4.017\n",
      "Epoch 630: Loss = 4.017\n",
      "Epoch 631: Loss = 4.017\n",
      "Epoch 632: Loss = 4.017\n",
      "Epoch 633: Loss = 4.017\n",
      "Epoch 634: Loss = 4.017\n",
      "Epoch 635: Loss = 4.017\n",
      "Epoch 636: Loss = 4.017\n",
      "Epoch 637: Loss = 4.017\n",
      "Epoch 638: Loss = 4.017\n",
      "Epoch 639: Loss = 4.017\n",
      "Epoch 640: Loss = 4.017\n",
      "Epoch 641: Loss = 4.017\n",
      "Epoch 642: Loss = 4.017\n",
      "Epoch 643: Loss = 4.017\n",
      "Epoch 644: Loss = 4.017\n",
      "Epoch 645: Loss = 4.017\n",
      "Epoch 646: Loss = 4.017\n",
      "Epoch 647: Loss = 4.017\n",
      "Epoch 648: Loss = 4.017\n",
      "Epoch 649: Loss = 4.017\n",
      "Epoch 650: Loss = 4.017\n",
      "Epoch 651: Loss = 4.017\n",
      "Epoch 652: Loss = 4.017\n",
      "Epoch 653: Loss = 4.017\n",
      "Epoch 654: Loss = 4.017\n",
      "Epoch 655: Loss = 4.017\n",
      "Epoch 656: Loss = 4.017\n",
      "Epoch 657: Loss = 4.017\n",
      "Epoch 658: Loss = 4.017\n",
      "Epoch 659: Loss = 4.017\n",
      "Epoch 660: Loss = 4.017\n",
      "Epoch 661: Loss = 4.017\n",
      "Epoch 662: Loss = 4.017\n",
      "Epoch 663: Loss = 4.017\n",
      "Epoch 664: Loss = 4.017\n",
      "Epoch 665: Loss = 4.017\n",
      "Epoch 666: Loss = 4.017\n",
      "Epoch 667: Loss = 4.017\n",
      "Epoch 668: Loss = 4.017\n",
      "Epoch 669: Loss = 4.017\n",
      "Epoch 670: Loss = 4.017\n",
      "Epoch 671: Loss = 4.017\n",
      "Epoch 672: Loss = 4.017\n",
      "Epoch 673: Loss = 4.017\n",
      "Epoch 674: Loss = 4.017\n",
      "Epoch 675: Loss = 4.017\n",
      "Epoch 676: Loss = 4.017\n",
      "Epoch 677: Loss = 4.017\n",
      "Epoch 678: Loss = 4.017\n",
      "Epoch 679: Loss = 4.017\n",
      "Epoch 680: Loss = 4.017\n",
      "Epoch 681: Loss = 4.017\n",
      "Epoch 682: Loss = 4.017\n",
      "Epoch 683: Loss = 4.017\n",
      "Epoch 684: Loss = 4.017\n",
      "Epoch 685: Loss = 4.017\n",
      "Epoch 686: Loss = 4.017\n",
      "Epoch 687: Loss = 4.017\n",
      "Epoch 688: Loss = 4.017\n",
      "Epoch 689: Loss = 4.017\n",
      "Epoch 690: Loss = 4.017\n",
      "Epoch 691: Loss = 4.017\n",
      "Epoch 692: Loss = 4.017\n",
      "Epoch 693: Loss = 4.017\n",
      "Epoch 694: Loss = 4.017\n",
      "Epoch 695: Loss = 4.017\n",
      "Epoch 696: Loss = 4.017\n",
      "Epoch 697: Loss = 4.017\n",
      "Epoch 698: Loss = 4.017\n",
      "Epoch 699: Loss = 4.017\n",
      "Epoch 700: Loss = 4.017\n",
      "Epoch 701: Loss = 4.017\n",
      "Epoch 702: Loss = 4.017\n",
      "Epoch 703: Loss = 4.017\n",
      "Epoch 704: Loss = 4.017\n",
      "Epoch 705: Loss = 4.017\n",
      "Epoch 706: Loss = 4.017\n",
      "Epoch 707: Loss = 4.017\n",
      "Epoch 708: Loss = 4.017\n",
      "Epoch 709: Loss = 4.017\n",
      "Epoch 710: Loss = 4.017\n",
      "Epoch 711: Loss = 4.017\n",
      "Epoch 712: Loss = 4.017\n",
      "Epoch 713: Loss = 4.017\n",
      "Epoch 714: Loss = 4.017\n",
      "Epoch 715: Loss = 4.017\n",
      "Epoch 716: Loss = 4.017\n",
      "Epoch 717: Loss = 4.017\n",
      "Epoch 718: Loss = 4.017\n",
      "Epoch 719: Loss = 4.017\n",
      "Epoch 720: Loss = 4.017\n",
      "Epoch 721: Loss = 4.017\n",
      "Epoch 722: Loss = 4.017\n",
      "Epoch 723: Loss = 4.017\n",
      "Epoch 724: Loss = 4.017\n",
      "Epoch 725: Loss = 4.017\n",
      "Epoch 726: Loss = 4.017\n",
      "Epoch 727: Loss = 4.017\n",
      "Epoch 728: Loss = 4.017\n",
      "Epoch 729: Loss = 4.017\n",
      "Epoch 730: Loss = 4.017\n",
      "Epoch 731: Loss = 4.017\n",
      "Epoch 732: Loss = 4.017\n",
      "Epoch 733: Loss = 4.017\n",
      "Epoch 734: Loss = 4.017\n",
      "Epoch 735: Loss = 4.017\n",
      "Epoch 736: Loss = 4.017\n",
      "Epoch 737: Loss = 4.017\n",
      "Epoch 738: Loss = 4.017\n",
      "Epoch 739: Loss = 4.017\n",
      "Epoch 740: Loss = 4.017\n",
      "Epoch 741: Loss = 4.017\n",
      "Epoch 742: Loss = 4.017\n",
      "Epoch 743: Loss = 4.017\n",
      "Epoch 744: Loss = 4.017\n",
      "Epoch 745: Loss = 4.017\n",
      "Epoch 746: Loss = 4.017\n",
      "Epoch 747: Loss = 4.017\n",
      "Epoch 748: Loss = 4.017\n",
      "Epoch 749: Loss = 4.017\n",
      "Epoch 750: Loss = 4.017\n",
      "Epoch 751: Loss = 4.017\n",
      "Epoch 752: Loss = 4.017\n",
      "Epoch 753: Loss = 4.017\n",
      "Epoch 754: Loss = 4.017\n",
      "Epoch 755: Loss = 4.017\n",
      "Epoch 756: Loss = 4.017\n",
      "Epoch 757: Loss = 4.017\n",
      "Epoch 758: Loss = 4.017\n",
      "Epoch 759: Loss = 4.017\n",
      "Epoch 760: Loss = 4.017\n",
      "Epoch 761: Loss = 4.017\n",
      "Epoch 762: Loss = 4.017\n",
      "Epoch 763: Loss = 4.017\n",
      "Epoch 764: Loss = 4.017\n",
      "Epoch 765: Loss = 4.017\n",
      "Epoch 766: Loss = 4.017\n",
      "Epoch 767: Loss = 4.017\n",
      "Epoch 768: Loss = 4.017\n",
      "Epoch 769: Loss = 4.017\n",
      "Epoch 770: Loss = 4.017\n",
      "Epoch 771: Loss = 4.017\n",
      "Epoch 772: Loss = 4.017\n",
      "Epoch 773: Loss = 4.017\n",
      "Epoch 774: Loss = 4.017\n",
      "Epoch 775: Loss = 4.017\n",
      "Epoch 776: Loss = 4.017\n",
      "Epoch 777: Loss = 4.017\n",
      "Epoch 778: Loss = 4.017\n",
      "Epoch 779: Loss = 4.017\n",
      "Epoch 780: Loss = 4.017\n",
      "Epoch 781: Loss = 4.017\n",
      "Epoch 782: Loss = 4.017\n",
      "Epoch 783: Loss = 4.017\n",
      "Epoch 784: Loss = 4.017\n",
      "Epoch 785: Loss = 4.017\n",
      "Epoch 786: Loss = 4.017\n",
      "Epoch 787: Loss = 4.017\n",
      "Epoch 788: Loss = 4.017\n",
      "Epoch 789: Loss = 4.017\n",
      "Epoch 790: Loss = 4.017\n",
      "Epoch 791: Loss = 4.017\n",
      "Epoch 792: Loss = 4.017\n",
      "Epoch 793: Loss = 4.017\n",
      "Epoch 794: Loss = 4.017\n",
      "Epoch 795: Loss = 4.017\n",
      "Epoch 796: Loss = 4.017\n",
      "Epoch 797: Loss = 4.017\n",
      "Epoch 798: Loss = 4.017\n",
      "Epoch 799: Loss = 4.017\n",
      "Epoch 800: Loss = 4.017\n",
      "Epoch 801: Loss = 4.017\n",
      "Epoch 802: Loss = 4.017\n",
      "Epoch 803: Loss = 4.017\n",
      "Epoch 804: Loss = 4.017\n",
      "Epoch 805: Loss = 4.017\n",
      "Epoch 806: Loss = 4.017\n",
      "Epoch 807: Loss = 4.017\n",
      "Epoch 808: Loss = 4.017\n",
      "Epoch 809: Loss = 4.017\n",
      "Epoch 810: Loss = 4.017\n",
      "Epoch 811: Loss = 4.017\n",
      "Epoch 812: Loss = 4.017\n",
      "Epoch 813: Loss = 4.017\n",
      "Epoch 814: Loss = 4.017\n",
      "Epoch 815: Loss = 4.017\n",
      "Epoch 816: Loss = 4.017\n",
      "Epoch 817: Loss = 4.017\n",
      "Epoch 818: Loss = 4.017\n",
      "Epoch 819: Loss = 4.017\n",
      "Epoch 820: Loss = 4.017\n",
      "Epoch 821: Loss = 4.017\n",
      "Epoch 822: Loss = 4.017\n",
      "Epoch 823: Loss = 4.017\n",
      "Epoch 824: Loss = 4.017\n",
      "Epoch 825: Loss = 4.017\n",
      "Epoch 826: Loss = 4.017\n",
      "Epoch 827: Loss = 4.017\n",
      "Epoch 828: Loss = 4.017\n",
      "Epoch 829: Loss = 4.017\n",
      "Epoch 830: Loss = 4.017\n",
      "Epoch 831: Loss = 4.017\n",
      "Epoch 832: Loss = 4.017\n",
      "Epoch 833: Loss = 4.017\n",
      "Epoch 834: Loss = 4.017\n",
      "Epoch 835: Loss = 4.017\n",
      "Epoch 836: Loss = 4.017\n",
      "Epoch 837: Loss = 4.017\n",
      "Epoch 838: Loss = 4.017\n",
      "Epoch 839: Loss = 4.017\n",
      "Epoch 840: Loss = 4.017\n",
      "Epoch 841: Loss = 4.017\n",
      "Epoch 842: Loss = 4.017\n",
      "Epoch 843: Loss = 4.017\n",
      "Epoch 844: Loss = 4.017\n",
      "Epoch 845: Loss = 4.017\n",
      "Epoch 846: Loss = 4.017\n",
      "Epoch 847: Loss = 4.017\n",
      "Epoch 848: Loss = 4.017\n",
      "Epoch 849: Loss = 4.017\n",
      "Epoch 850: Loss = 4.017\n",
      "Epoch 851: Loss = 4.017\n",
      "Epoch 852: Loss = 4.017\n",
      "Epoch 853: Loss = 4.017\n",
      "Epoch 854: Loss = 4.017\n",
      "Epoch 855: Loss = 4.017\n",
      "Epoch 856: Loss = 4.017\n",
      "Epoch 857: Loss = 4.017\n",
      "Epoch 858: Loss = 4.017\n",
      "Epoch 859: Loss = 4.017\n",
      "Epoch 860: Loss = 4.017\n",
      "Epoch 861: Loss = 4.017\n",
      "Epoch 862: Loss = 4.017\n",
      "Epoch 863: Loss = 4.017\n",
      "Epoch 864: Loss = 4.017\n",
      "Epoch 865: Loss = 4.017\n",
      "Epoch 866: Loss = 4.017\n",
      "Epoch 867: Loss = 4.017\n",
      "Epoch 868: Loss = 4.017\n",
      "Epoch 869: Loss = 4.017\n",
      "Epoch 870: Loss = 4.017\n",
      "Epoch 871: Loss = 4.017\n",
      "Epoch 872: Loss = 4.017\n",
      "Epoch 873: Loss = 4.017\n",
      "Epoch 874: Loss = 4.017\n",
      "Epoch 875: Loss = 4.017\n",
      "Epoch 876: Loss = 4.017\n",
      "Epoch 877: Loss = 4.017\n",
      "Epoch 878: Loss = 4.017\n",
      "Epoch 879: Loss = 4.017\n",
      "Epoch 880: Loss = 4.017\n",
      "Epoch 881: Loss = 4.017\n",
      "Epoch 882: Loss = 4.017\n",
      "Epoch 883: Loss = 4.017\n",
      "Epoch 884: Loss = 4.017\n",
      "Epoch 885: Loss = 4.017\n",
      "Epoch 886: Loss = 4.017\n",
      "Epoch 887: Loss = 4.017\n",
      "Epoch 888: Loss = 4.017\n",
      "Epoch 889: Loss = 4.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 890: Loss = 4.017\n",
      "Epoch 891: Loss = 4.017\n",
      "Epoch 892: Loss = 4.017\n",
      "Epoch 893: Loss = 4.017\n",
      "Epoch 894: Loss = 4.017\n",
      "Epoch 895: Loss = 4.017\n",
      "Epoch 896: Loss = 4.017\n",
      "Epoch 897: Loss = 4.017\n",
      "Epoch 898: Loss = 4.017\n",
      "Epoch 899: Loss = 4.017\n",
      "Epoch 900: Loss = 4.017\n",
      "Epoch 901: Loss = 4.017\n",
      "Epoch 902: Loss = 4.017\n",
      "Epoch 903: Loss = 4.017\n",
      "Epoch 904: Loss = 4.017\n",
      "Epoch 905: Loss = 4.017\n",
      "Epoch 906: Loss = 4.017\n",
      "Epoch 907: Loss = 4.017\n",
      "Epoch 908: Loss = 4.017\n",
      "Epoch 909: Loss = 4.017\n",
      "Epoch 910: Loss = 4.017\n",
      "Epoch 911: Loss = 4.017\n",
      "Epoch 912: Loss = 4.017\n",
      "Epoch 913: Loss = 4.017\n",
      "Epoch 914: Loss = 4.017\n",
      "Epoch 915: Loss = 4.017\n",
      "Epoch 916: Loss = 4.017\n",
      "Epoch 917: Loss = 4.017\n",
      "Epoch 918: Loss = 4.017\n",
      "Epoch 919: Loss = 4.017\n",
      "Epoch 920: Loss = 4.017\n",
      "Epoch 921: Loss = 4.017\n",
      "Epoch 922: Loss = 4.017\n",
      "Epoch 923: Loss = 4.017\n",
      "Epoch 924: Loss = 4.017\n",
      "Epoch 925: Loss = 4.017\n",
      "Epoch 926: Loss = 4.017\n",
      "Epoch 927: Loss = 4.017\n",
      "Epoch 928: Loss = 4.017\n",
      "Epoch 929: Loss = 4.017\n",
      "Epoch 930: Loss = 4.017\n",
      "Epoch 931: Loss = 4.017\n",
      "Epoch 932: Loss = 4.017\n",
      "Epoch 933: Loss = 4.017\n",
      "Epoch 934: Loss = 4.017\n",
      "Epoch 935: Loss = 4.017\n",
      "Epoch 936: Loss = 4.017\n",
      "Epoch 937: Loss = 4.017\n",
      "Epoch 938: Loss = 4.017\n",
      "Epoch 939: Loss = 4.017\n",
      "Epoch 940: Loss = 4.017\n",
      "Epoch 941: Loss = 4.017\n",
      "Epoch 942: Loss = 4.017\n",
      "Epoch 943: Loss = 4.017\n",
      "Epoch 944: Loss = 4.017\n",
      "Epoch 945: Loss = 4.017\n",
      "Epoch 946: Loss = 4.017\n",
      "Epoch 947: Loss = 4.017\n",
      "Epoch 948: Loss = 4.017\n",
      "Epoch 949: Loss = 4.017\n",
      "Epoch 950: Loss = 4.017\n",
      "Epoch 951: Loss = 4.017\n",
      "Epoch 952: Loss = 4.017\n",
      "Epoch 953: Loss = 4.017\n",
      "Epoch 954: Loss = 4.017\n",
      "Epoch 955: Loss = 4.017\n",
      "Epoch 956: Loss = 4.017\n",
      "Epoch 957: Loss = 4.017\n",
      "Epoch 958: Loss = 4.017\n",
      "Epoch 959: Loss = 4.017\n",
      "Epoch 960: Loss = 4.017\n",
      "Epoch 961: Loss = 4.017\n",
      "Epoch 962: Loss = 4.017\n",
      "Epoch 963: Loss = 4.017\n",
      "Epoch 964: Loss = 4.017\n",
      "Epoch 965: Loss = 4.017\n",
      "Epoch 966: Loss = 4.017\n",
      "Epoch 967: Loss = 4.017\n",
      "Epoch 968: Loss = 4.017\n",
      "Epoch 969: Loss = 4.017\n",
      "Epoch 970: Loss = 4.017\n",
      "Epoch 971: Loss = 4.017\n",
      "Epoch 972: Loss = 4.017\n",
      "Epoch 973: Loss = 4.017\n",
      "Epoch 974: Loss = 4.017\n",
      "Epoch 975: Loss = 4.017\n",
      "Epoch 976: Loss = 4.017\n",
      "Epoch 977: Loss = 4.017\n",
      "Epoch 978: Loss = 4.017\n",
      "Epoch 979: Loss = 4.017\n",
      "Epoch 980: Loss = 4.017\n",
      "Epoch 981: Loss = 4.017\n",
      "Epoch 982: Loss = 4.017\n",
      "Epoch 983: Loss = 4.017\n",
      "Epoch 984: Loss = 4.017\n",
      "Epoch 985: Loss = 4.017\n",
      "Epoch 986: Loss = 4.017\n",
      "Epoch 987: Loss = 4.017\n",
      "Epoch 988: Loss = 4.017\n",
      "Epoch 989: Loss = 4.017\n",
      "Epoch 990: Loss = 4.017\n",
      "Epoch 991: Loss = 4.017\n",
      "Epoch 992: Loss = 4.017\n",
      "Epoch 993: Loss = 4.017\n",
      "Epoch 994: Loss = 4.017\n",
      "Epoch 995: Loss = 4.017\n",
      "Epoch 996: Loss = 4.017\n",
      "Epoch 997: Loss = 4.017\n",
      "Epoch 998: Loss = 4.017\n",
      "Epoch 999: Loss = 4.017\n",
      "Epoch 1000: Loss = 4.017\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "max_epoch = 1000\n",
    "w, hist_loss = train_model(X_train, y_train, alpha, max_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5ad9e",
   "metadata": {},
   "source": [
    "#### Display the estimated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c6f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated weights:\n",
      "The estimated weight of w0 is: 9.544216925779185\n",
      "The estimated weight of w1 is: 11.748308254185522\n",
      "The estimated weight of w2 is: -0.07320023114136123\n",
      "The estimated weight of w3 is: 0.10463248526216076\n",
      "The estimated weight of w4 is: 36.97281531420788\n",
      "The estimated weight of w5 is: 0.03465440468233703\n"
     ]
    }
   ],
   "source": [
    "# This section will print the estimated weight that are already calculated from train_model fuunction\n",
    "\n",
    "print(\"Estimated weights:\")\n",
    "for i, weight in enumerate(w):\n",
    "    print(f\"The estimated weight of w{i} is: {weight:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47476f87",
   "metadata": {},
   "source": [
    "#### Display the training loss against epoch graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "004eb58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss against epoch\n",
    "plt.plot(range(max_epoch), hist_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ac2cf",
   "metadata": {},
   "source": [
    "#### Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bff3adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.111794879906295\n"
     ]
    }
   ],
   "source": [
    "# Calculate the test loss using the estimated weights\n",
    "yhat_test = predict(w, X_test)\n",
    "test_loss = loss_fn(y_test, yhat_test)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f206de",
   "metadata": {},
   "source": [
    "#### Display the r2 score, mean squared error and mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecddf85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c05fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R-squared: 0.9834333413966764\n",
      "Test set MSE: 27.294250357419305\n",
      "Test set MAE: 4.111794879906295\n"
     ]
    }
   ],
   "source": [
    "# Calculate the test set performance metrics\n",
    "r_squared = r2_score(y_test, yhat_test)\n",
    "mse = mean_squared_error(y_test, yhat_test)\n",
    "mae = mean_absolute_error(y_test, yhat_test)\n",
    "\n",
    "# Display the performance metrics\n",
    "print(\"Test set R-squared:\", r_squared)\n",
    "print(\"Test set MSE:\", mse)\n",
    "print(\"Test set MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41448145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you and have a nice day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
